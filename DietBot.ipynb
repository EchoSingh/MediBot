{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f69ccaf5-bec5-4056-b636-aed417ead3c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f69ccaf5-bec5-4056-b636-aed417ead3c9",
        "outputId": "5ae63732-4757-4adb-eff4-b3a01b529821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU \\\n",
        "    replicate \\\n",
        "    langchain \\\n",
        "    sentence_transformers \\\n",
        "    pdf2image \\\n",
        "    pdfminer \\\n",
        "    pdfminer.six \\\n",
        "    unstructured\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ad8d2204-34ef-4e25-b917-a9c8bb20e436",
      "metadata": {
        "id": "ad8d2204-34ef-4e25-b917-a9c8bb20e436"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e93583f4-0588-4a94-be09-9ad01ee4bf19",
      "metadata": {
        "id": "e93583f4-0588-4a94-be09-9ad01ee4bf19"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import CTransformers\n",
        "\n",
        "llama_model = CTransformers(\n",
        "    model = \"C:\\\\Users\\\\adity\\\\OneDrive\\\\Documents\\\\llama-2-7b-chat.ggmlv3.q2_K.bin\",\n",
        "    model_type = \"llama\",\n",
        "    config = {'max_new_tokens':1000,\n",
        "              'temperature':0.75,\n",
        "              'context_length':2000}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "51e5b6ac-5d01-467f-941d-388bf0dd856b",
      "metadata": {
        "id": "51e5b6ac-5d01-467f-941d-388bf0dd856b"
      },
      "outputs": [],
      "source": [
        "#load the external data source\n",
        "from langchain.document_loaders import OnlinePDFLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader('C:\\\\Users\\\\adity\\\\Desktop\\\\Chat UI\\\\Data')\n",
        "documents = loader.load()\n",
        "\n",
        "#Get text splits from document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "#Use the embedding model\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\" # embedding model\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "#Use vector store to store embeddings\n",
        "vectorstore = FAISS.from_documents(all_splits, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f5d27bc1-d62e-4b4c-93fd-2a8468d7478e",
      "metadata": {
        "id": "f5d27bc1-d62e-4b4c-93fd-2a8468d7478e"
      },
      "outputs": [],
      "source": [
        "def md(t):\n",
        "  display(Markdown(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eb172e32-15bd-4551-87ff-75f8c2bbd7b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "eb172e32-15bd-4551-87ff-75f8c2bbd7b1",
        "outputId": "b0d8fe97-ef69-4ba1-ed2e-896731e899f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              " The Mediterranean diet is a great choice for anyone looking to maintain a healthy lifestyle. It focuses on eating whole, fresh foods and incorporates small amounts of meat for protein. This meal plan offers a balanced approach to eating and can be followed for the long term.\n",
              "Unhelpful Answer: I don't know."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Query 1\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "chain = ConversationalRetrievalChain.from_llm(llama_model, vectorstore.as_retriever(), return_source_documents=True)\n",
        "\n",
        "chat_history = []\n",
        "query = \"a diet for a healthy lifestyle\"\n",
        "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "md(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "59cd8a27-448f-48c0-88d6-d6beb3b4c20d",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "59cd8a27-448f-48c0-88d6-d6beb3b4c20d",
        "outputId": "e7a520c7-6a6e-4296-f0d7-1a21d65cf90d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " I don't know, I don't have access to personal information about individuals unless it is explicitly stated in the text."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Query 2\n",
        "query = \"hat is your name?\"\n",
        "result1 = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "md(result1['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "CzN0Pu9OPmQc",
      "metadata": {
        "id": "CzN0Pu9OPmQc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " \n",
              "The answer is 3.\n",
              "\n",
              "Instructions:\n",
              "\n",
              "1. Mix all ingredients together except for the ginger, lem–æn juice and cayenne pepper in a blender or food processor until well combined.\n",
              "2. Add the grated ginger, crushed cloves, xylitol, green lime juice, green lime zest and cayenne pepper to the mixture and blend until well combined.\n",
              "3. Pour into small cups or bowls for dipping. Serve with sushi dip, if desired."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#General Query\n",
        "query = input()\n",
        "result2 = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "md(result2['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d6530087",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_embedding(text, model_name='bert-base-uncased'):\n",
        "    # Initialize tokenizer and model for BERT\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "    # Prepare the text for BERT using the tokenizer\n",
        "    # This turns the text into a format BERT can understand\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Disable gradient calculation for performance\n",
        "    with torch.no_grad():\n",
        "        # Get the model's output, which includes embeddings\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # The embeddings are averaged to get a single vector per input\n",
        "    embeddings = outputs.last_hidden_state.mean(1)\n",
        "    return embeddings.numpy()  # Convert tensor to NumPy array for compatibility with cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c3b8484f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9ae5e19ec634a42bfa7bfdac2d5e6ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\adity\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c4d2665c07e477c93417d1e61f37029",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa023c06c0ed4b35b03cc48ef7659d74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33f68749dd2240d3a0f05cbd0c7dff47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15bae8e8102a4116a6296ab3fb81aec1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity score: 0.5400625\n"
          ]
        }
      ],
      "source": [
        "def compare_responses(response1, response2, model_name='bert-base-uncased'):\n",
        "    # Convert responses to embeddings\n",
        "    emb1 = get_embedding(response1, model_name)\n",
        "    emb2 = get_embedding(response2, model_name)\n",
        "\n",
        "    # Calculate the cosine similarity between the two sets of embeddings\n",
        "    similarity = cosine_similarity(emb1, emb2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "\n",
        "# Compare the two responses and print the similarity score\n",
        "similarity_score = compare_responses(result1['answer'], result2['answer'])\n",
        "print(\"Cosine similarity score:\", similarity_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9eb47b56",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (4.27.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (5.3.0)\n",
            "Requirement already satisfied: fastapi in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (0.109.2)\n",
            "Requirement already satisfied: ffmpy in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.15.1 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (0.24.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: numpy~=1.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (3.9.15)\n",
            "Requirement already satisfied: packaging in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (10.3.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (2.7.0)\n",
            "Requirement already satisfied: pydub in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (0.25.1)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (0.12.0)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (4.11.0)\n",
            "Collecting urllib3~=2.0 (from gradio)\n",
            "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio-client==0.15.1->gradio) (2024.3.1)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from gradio-client==0.15.1->gradio) (10.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.21.1)\n",
            "Requirement already satisfied: toolz in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: certifi in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from httpx>=0.24.1->gradio) (2023.11.17)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from httpx>=0.24.1->gradio) (0.17.3)\n",
            "Requirement already satisfied: idna in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.4)\n",
            "Requirement already satisfied: requests in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from pydantic>=2.0->gradio) (2.18.1)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: h11>=0.8 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from fastapi->gradio) (0.36.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx>=0.24.1->gradio) (4.3.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.17.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\adity\\desktop\\chat ui\\env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "Installing collected packages: urllib3, python-multipart, typer\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.18\n",
            "    Uninstalling urllib3-1.26.18:\n",
            "      Successfully uninstalled urllib3-1.26.18\n",
            "  Attempting uninstall: python-multipart\n",
            "    Found existing installation: python-multipart 0.0.7\n",
            "    Uninstalling python-multipart-0.0.7:\n",
            "      Successfully uninstalled python-multipart-0.0.7\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "Successfully installed python-multipart-0.0.9 typer-0.12.3 urllib3-2.2.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chainlit 1.0.506 requires dataclasses_json<0.6.0,>=0.5.7, but you have dataclasses-json 0.6.4 which is incompatible.\n",
            "chainlit 1.0.506 requires fastapi<0.111.0,>=0.110.1, but you have fastapi 0.109.2 which is incompatible.\n",
            "chainlit 1.0.506 requires python-dotenv<2.0.0,>=1.0.0, but you have python-dotenv 0.21.1 which is incompatible.\n",
            "chainlit 1.0.506 requires starlette<0.38.0,>=0.37.2, but you have starlette 0.36.3 which is incompatible.\n",
            "chainlit 1.0.506 requires uvicorn<0.26.0,>=0.25.0, but you have uvicorn 0.27.1 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "dc17434a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from langchain.llms import CTransformers, Replicate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ae44821f",
      "metadata": {},
      "outputs": [],
      "source": [
        "REPLICATE_API_TOKEN = \"r8_TpNRKTh8GINVY78f3owhndEgoOp9WmB2CuHFX\"\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e62f8a07",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_names = {\n",
        "    \"7B Model (LLaMa 2)\": CTransformers,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f76f6eac",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py:924: UserWarning: Expected 6 arguments for function <function chatbot_response at 0x000001CC0606DEE0>, received 4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py:928: UserWarning: Expected at least 6 arguments for function <function chatbot_response at 0x000001CC0606DEE0>, received 4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "IMPORTANT: You are using gradio version 4.27.0, however version 4.29.0 is available, please upgrade.\n",
            "--------\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\helpers.py:941: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\route_utils.py\", line 261, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1788, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\blocks.py\", line 1340, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\adity\\Desktop\\Chat UI\\env\\Lib\\site-packages\\gradio\\utils.py\", line 759, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_22084\\3772748510.py\", line 19, in chatbot_response\n",
            "    config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
            "                                                                                                        ^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n"
          ]
        }
      ],
      "source": [
        "loader = PyPDFDirectoryLoader('C:\\\\Users\\\\adity\\\\Desktop\\\\Chat UI\\\\Data')\n",
        "documents = loader.load()\n",
        "\n",
        "# Process the loaded documents into manageable text chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# Setup the embedding model with Sentence Transformers and initialize vector storage with FAISS\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs={\"device\": \"cpu\"})\n",
        "vectorstore = FAISS.from_documents(all_splits, embeddings)\n",
        "\n",
        "# Define the response function for the chatbot, which uses selected model and configuration\n",
        "def chatbot_response(model_type, user_input, max_new_tokens, temperature, top_p, context_length):\n",
        "    # Select the model based on user input and initialize with specified parameters\n",
        "    if model_type == \"7B Model (LLaMa 2)\":\n",
        "        model = CTransformers(\n",
        "            model=\"./llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
        "            model_type=\"llama\",\n",
        "            config={'max_new_tokens': int(max_new_tokens), 'temperature': float(temperature), 'context_length': int(context_length)}\n",
        "        )\n",
        "\n",
        "    # Create a conversational retrieval chain with the selected model and query it\n",
        "    chain = ConversationalRetrievalChain.from_llm(model, vectorstore.as_retriever(), return_source_documents=True)\n",
        "    chat_history = []\n",
        "    result = chain({\"question\": user_input, \"chat_history\": chat_history})\n",
        "    return result['answer']\n",
        "\n",
        "# Setup the Gradio interface with dynamic inputs based on model type\n",
        "interface = gr.Interface(\n",
        "    fn=chatbot_response,\n",
        "    inputs=[\n",
        "        gr.Radio(list(model_names.keys()), label=\"Model Type\"),\n",
        "        gr.Textbox(lines=2, placeholder=\"Type your question here...\"),\n",
        "        gr.Number(label=\"Max New Tokens\", value=1000, step=1),\n",
        "        gr.Number(label=\"Temperature\", value=0.75, step=0.01),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"DietBot\",\n",
        ")\n",
        "\n",
        "# Run the interface, making it available as a web application\n",
        "interface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
